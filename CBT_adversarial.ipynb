{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224N - CBT Adversarial Dataset Evaluation\n",
    "Evaluating our methods on an adversarial dataset of CBT examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using GPU: \" + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in 1:1 Real to Fake Words\n",
    "Load in from the `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import CSV and get the real to fake\n",
    "rtf_df = pd.read_csv(\"datasets/realtofake.csv\")\n",
    "real_words_list = rtf_df[\"Real\"].tolist()\n",
    "fake_words_list = rtf_df[\"Fake\"].tolist()\n",
    "\n",
    "# Populate dictionary\n",
    "real_to_fake_dict = {}\n",
    "for i in range(len(real_words_list)):\n",
    "    real_to_fake_dict[real_words_list[i]] = fake_words_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Definitions for all of the WinoDict Words\n",
    "Get each of the definitions for the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Keep track of all the final definitions\n",
    "final_definitions = []\n",
    "\n",
    "# Loop through each real word and append\n",
    "for word in real_words_list:\n",
    "    definition = \"\"\n",
    "    for synset in wn.synsets(word):\n",
    "        definition += synset.definition() + \". \"\n",
    "    final_definitions.append(definition)\n",
    "\n",
    "# Quick sanity check\n",
    "assert(len(real_words_list) == len(final_definitions))\n",
    "assert(len(fake_words_list) == len(final_definitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prediction and Actual Model\n",
    "Retrieve G2G and R2G and baseline GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# G2G/R2G used to Predict\n",
    "predict_model = GPT2LMHeadModel.from_pretrained(\"weights/G2GNext1\").to(\"cuda\")\n",
    "predict_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "predict_tokenizer.add_tokens(['[CLS]'])\n",
    "\n",
    "# GPT-2 Model and Tokenizer to be fine-tuned\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium').to(\"cuda\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "\n",
    "# Standalone tokenizer\n",
    "ORIG_TOKENIZER = GPT2Tokenizer.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch All Embeddings\n",
    "Predict all of the embeddings for each of the definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total definitions: 343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helpful Debug Message\n",
    "print(\"Number of total definitions: \" + str(len(final_definitions)))\n",
    "\n",
    "# Tokenizing all of the definitions at once\n",
    "predict_tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_inputs = predict_tokenizer(final_definitions, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=511)\n",
    "tokenized_cls = predict_tokenizer([\" [CLS]\"] * len(final_definitions), return_tensors=\"pt\")\n",
    "tokenized_inputs['input_ids'] = torch.cat((tokenized_inputs['input_ids'], tokenized_cls['input_ids']), dim=1).to(\"cuda\")\n",
    "\n",
    "# Add the new tokens and resize the model embeddings matrix\n",
    "displacement = len(tokenizer)\n",
    "tokenizer.add_tokens(fake_words_list)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "params = model.state_dict()\n",
    "\n",
    "# Adding new embeddings in a range of 4\n",
    "for i in range(0, len(final_definitions), 4):\n",
    "    outputs = predict_model(input_ids=tokenized_inputs['input_ids'][i:min(len(final_definitions), i + 4)], output_hidden_states=True)\n",
    "    params['transformer.wte.weight'][displacement + i: displacement + min(len(final_definitions), i + 4),:] = outputs.hidden_states[-1][:,511,:].detach().clone()\n",
    "model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Retrieving Fine-Tuned Model\n",
    "Use this to retrieve the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_model():\n",
    "    from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"weights/GWithFineTunedSentencesG2GInitializationT\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"weights/GWithFineTunedSentencesG2GInitialization\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "first_set = pd.read_csv(\"winodict/prob1_of_5.csv\")\n",
    "real_words_list = first_set['lemma'].tolist()\n",
    "fake_words_list = first_set[\"fake_lemma\"].tolist()\n",
    "real_to_fake_dict = {}\n",
    "for i in range(len(real_words_list)):\n",
    "    if real_words_list[i] in real_to_fake_dict:\n",
    "        real_to_fake_dict[real_words_list[i]].append(fake_words_list[i])\n",
    "    else:\n",
    "        real_to_fake_dict[real_words_list[i]] = [fake_words_list[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "cbt_df = pd.read_csv(\"updated_cbt_extract/updated_cbt_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cbt_eval_info/ADVERSARIAL_indices\", \"r\") as f:\n",
    "    adversarial_indices = f.read()\n",
    "\n",
    "adversarial_indices = adversarial_indices[1:len(adversarial_indices)-1].split(\", \")\n",
    "for i in range(len(adversarial_indices)):\n",
    "    adversarial_indices[i] = int(adversarial_indices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cbt(example):\n",
    "    # extract the sentence\n",
    "    sentence = example[\"sentences\"]\n",
    "    sentence = sentence.replace('\"','')\n",
    "    sentence = sentence.replace(\"\\n\",\"\")\n",
    "    sentence = sentence[1:len(sentence)-1]\n",
    "\n",
    "    # extract the question\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    # make the total query with the sentence and the question\n",
    "    total_query = sentence + \" \" + question\n",
    "\n",
    "    \n",
    "    #replace words in example with winodict fake words (currently, we replace all possible identified examples)\n",
    "    replace_words = example[\"winodict_words\"]\n",
    "    replace_words = replace_words[1:len(replace_words)-1].replace(\"'\",\"\").split(\",\")\n",
    "\n",
    "    for word in replace_words:\n",
    "        word = word.strip()\n",
    "        word = \" \"+word+\" \"\n",
    "        while word in total_query:\n",
    "            total_query = total_query.replace(word,\" \"+real_to_fake_dict[word.strip()][0]+\" \")\n",
    "    \n",
    "\n",
    "    # get the answer options for the model into a list\n",
    "    options = example[\"options\"].replace(\"'\",\"\")\n",
    "    options = options.replace(\"\\n\",\"\")\n",
    "    options = options[1:len(options)-1].split(\" \")\n",
    "\n",
    "    # get the answer to the problem\n",
    "    answer = example[\"answer\"]\n",
    "\n",
    "    \n",
    "    if answer in replace_words:\n",
    "        answer = real_to_fake_dict[answer][0]\n",
    "    \n",
    "        \n",
    "\n",
    "    # initialize best answer and best loss (will be lowest value)\n",
    "    best_answer = \"\"\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    # compute the loss for each option (using full scoring)\n",
    "    for option in options:\n",
    "        \n",
    "        if option in replace_words:\n",
    "            option = real_to_fake_dict[option][0]\n",
    "        \n",
    "\n",
    "        updated_query = total_query.replace(\"XXXXX\", option)\n",
    "        \n",
    "        # Tokenize each string and produce labels\n",
    "        updated_input = tokenizer(updated_query, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        if len(updated_input[0]) > 1024:\n",
    "            return -1\n",
    "\n",
    "        current_loss = model(**updated_input, labels=updated_input[\"input_ids\"].to(\"cuda\")).loss\n",
    "        \n",
    "        if current_loss < best_loss:\n",
    "            best_answer = option\n",
    "            best_loss = current_loss\n",
    "    return best_answer == answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 22.20 GiB total capacity; 8.03 GiB already allocated; 14.06 MiB free; 8.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m adversarial_indices:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m update \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_cbt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 59\u001b[0m, in \u001b[0;36mevaluate_cbt\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(updated_input[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1024\u001b[39m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 59\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mupdated_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdated_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_loss \u001b[38;5;241m<\u001b[39m best_loss:\n\u001b[1;32m     62\u001b[0m     best_answer \u001b[38;5;241m=\u001b[39m option\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1043\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1034'>1035</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1035'>1036</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1036'>1037</a>\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1037'>1038</a>\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1038'>1039</a>\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1039'>1040</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1040'>1041</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1042'>1043</a>\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1043'>1044</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1044'>1045</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1045'>1046</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1046'>1047</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1047'>1048</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1048'>1049</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1049'>1050</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1050'>1051</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1051'>1052</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1052'>1053</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1053'>1054</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1054'>1055</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1055'>1056</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1056'>1057</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1057'>1058</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1059'>1060</a>\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:887\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=876'>877</a>\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=877'>878</a>\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=878'>879</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=883'>884</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=884'>885</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=885'>886</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=886'>887</a>\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=887'>888</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=888'>889</a>\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=889'>890</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=890'>891</a>\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=891'>892</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=892'>893</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=893'>894</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=894'>895</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=895'>896</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=897'>898</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=898'>899</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:425\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=422'>423</a>\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=423'>424</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=424'>425</a>\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=425'>426</a>\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=426'>427</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:353\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=350'>351</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[39m.\u001b[39mFloatTensor]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=351'>352</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m--> <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=352'>353</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(hidden_states)\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=353'>354</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(hidden_states)\n\u001b[1;32m    <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=354'>355</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/activations.py:35\u001b[0m, in \u001b[0;36mNewGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/activations.py?line=33'>34</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> <a href='file:///home/ubuntu/.local/lib/python3.9/site-packages/transformers/activations.py?line=34'>35</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtanh(math\u001b[39m.\u001b[39msqrt(\u001b[39m2.0\u001b[39m \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39mpi) \u001b[39m*\u001b[39m (\u001b[39minput\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.044715\u001b[39;49m \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49mpow(\u001b[39minput\u001b[39;49m, \u001b[39m3.0\u001b[39;49m))))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 22.20 GiB total capacity; 8.03 GiB already allocated; 14.06 MiB free; 8.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "correct_replaced_list = []\n",
    "total_replaced_list = []\n",
    "\n",
    "correct = 0 \n",
    "total = 0 \n",
    "for index, row in cbt_df.iterrows():\n",
    "    print(index)\n",
    "\n",
    "    if index not in adversarial_indices:\n",
    "        continue\n",
    "\n",
    "    update = evaluate_cbt(row)\n",
    "    if update == -1:\n",
    "        continue\n",
    "    if update == 1:\n",
    "        correct_replaced_list.append(index)\n",
    "    total_replaced_list.append(index)\n",
    "    correct += update\n",
    "    total += 1\n",
    "\n",
    "correct_replaced_list = str(correct_replaced_list)\n",
    "total_replaced_list = str(total_replaced_list)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Number correct: \", correct)\n",
    "print(\"Total: \", total)\n",
    "print(\"Correct percentage: \",correct/total)\n",
    "\n",
    "information_list = str([correct, total, correct/total])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
