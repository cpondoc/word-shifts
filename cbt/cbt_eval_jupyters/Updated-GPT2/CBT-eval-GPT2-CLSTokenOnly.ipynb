{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224N Final Project - Evaluating on CBT Dataset\n",
    "By: Joseph O'Brien, Christopher Pondoc, Joseph Guman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using GPU: \" + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in 1:1 Real to Fake Words\n",
    "Load in from the `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import CSV and get the real to fake\n",
    "rtf_df = pd.read_csv(\"/home/ubuntu/test/datasets/realtofake.csv\")\n",
    "real_words_list = rtf_df[\"Real\"].tolist()\n",
    "fake_words_list = rtf_df[\"Fake\"].tolist()\n",
    "\n",
    "# Populate dictionary\n",
    "real_to_fake_dict = {}\n",
    "for i in range(len(real_words_list)):\n",
    "    real_to_fake_dict[real_words_list[i]] = fake_words_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Definitions for all of the WinoDict Words\n",
    "Get each of the definitions for the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Keep track of all the final definitions\n",
    "final_definitions = []\n",
    "\n",
    "# Loop through each real word and append\n",
    "for word in real_words_list:\n",
    "    definition = \"\"\n",
    "    for synset in wn.synsets(word):\n",
    "        definition += synset.definition() + \". \"\n",
    "    final_definitions.append(definition)\n",
    "\n",
    "# Quick sanity check\n",
    "assert(len(real_words_list) == len(final_definitions))\n",
    "assert(len(fake_words_list) == len(final_definitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_sentence_list = [] \n",
    "\n",
    "for i in range(len(real_words_list)):\n",
    "    same_sentence_list.append(\"This is a very normal sentence that will have normal results to give to someting\")\n",
    "\n",
    "assert(len(fake_words_list) == len(real_words_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prediction and Actual Model\n",
    "Retrieve G2G and R2G and baseline GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# G2G/R2G used to Predict\n",
    "predict_model = GPT2LMHeadModel.from_pretrained(\"/home/ubuntu/test/weights/G2GSamplingM\").to(\"cuda\")\n",
    "predict_tokenizer = GPT2Tokenizer.from_pretrained(\"/home/ubuntu/test/weights/G2GSamplingT\")\n",
    "\n",
    "# GPT-2 Model and Tokenizer to be fine-tuned\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium').to(\"cuda\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch All Embeddings\n",
    "Predict all of the embeddings for each of the definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total definitions: 343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helpful Debug Message\n",
    "print(\"Number of total definitions: \" + str(len(final_definitions)))\n",
    "\n",
    "# Tokenizing all of the definitions at once\n",
    "#tokenized_inputs = predict_tokenizer(final_definitions, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=511)\n",
    "tokenized_inputs = predict_tokenizer(same_sentence_list, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=511)\n",
    "tokenized_cls = predict_tokenizer([\" [CLS]\"] * len(final_definitions), return_tensors=\"pt\")\n",
    "\n",
    "# Get the correct input IDs and and attention mask\n",
    "tokenized_inputs['input_ids'] = torch.cat((tokenized_inputs['input_ids'], tokenized_cls['input_ids']), dim=1).to(\"cuda\")\n",
    "tokenized_inputs['attention_mask'] = torch.cat((tokenized_inputs['attention_mask'], tokenized_cls['attention_mask']), dim=1).to(\"cuda\")\n",
    "\n",
    "# Add the new tokens and resize the model embeddings matrix\n",
    "displacement = len(tokenizer)\n",
    "tokenizer.add_tokens(fake_words_list)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "params = model.state_dict()\n",
    "\n",
    "# Adding new embeddings in a range of 4\n",
    "for i in range(0, len(final_definitions), 4):\n",
    "    outputs = predict_model(input_ids=tokenized_inputs['input_ids'][i:min(len(final_definitions), i + 4)], output_hidden_states=True, attention_mask=tokenized_inputs['attention_mask'][i:min(len(final_definitions), i + 4)])\n",
    "    params['transformer.wte.weight'][displacement + i: displacement + min(len(final_definitions), i + 4),:] = outputs.hidden_states[-1][:,511,:].detach().clone()\n",
    "model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del predict_model\n",
    "del predict_tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.1018e-02, -1.0472e-02, -2.6941e-02, -9.4060e-02, -3.8462e-02,\n",
      "         8.0635e-02, -4.2176e-02, -6.7524e-02, -3.2577e-02,  1.6920e-01,\n",
      "        -6.7162e-03,  1.1194e-02, -5.2289e-02, -3.4464e-02, -8.0983e-03,\n",
      "         3.1571e-02,  8.9290e-02, -1.7303e-01, -5.9624e-02,  7.2877e-02,\n",
      "        -2.6239e-02, -5.4489e-03,  3.9657e-02,  1.9218e-03,  1.2603e-02,\n",
      "        -1.8843e-02,  4.0793e-02,  4.6155e-02,  1.7052e-02, -4.4290e-02,\n",
      "        -3.1765e-02,  2.9278e-02,  9.9705e-02, -2.1224e-02,  2.9825e-02,\n",
      "        -3.3536e-02,  2.1350e-02,  1.2614e-02, -1.1166e-02,  7.4746e-02,\n",
      "        -4.6766e-02, -6.5998e-03,  2.2396e-02,  1.7701e-02,  1.2839e-02,\n",
      "        -2.1378e-02,  5.9309e-02, -7.8575e-03,  1.1850e-01,  4.0888e-02,\n",
      "        -9.7272e-02,  8.8698e-02, -3.2979e-02,  8.4463e-04, -1.5666e-02,\n",
      "        -4.9736e-02,  7.3885e-02, -1.7835e-02, -8.3907e-02,  4.1915e-02,\n",
      "        -1.4975e-03,  7.7594e-02,  4.5326e-02, -4.3028e-02,  2.9704e-02,\n",
      "         3.9824e-02, -5.2617e-02, -1.6799e-02,  1.1812e-02, -3.5314e-02,\n",
      "         1.0355e-02, -4.8763e-02,  4.8434e-02,  1.9016e-03,  6.8001e-02,\n",
      "        -8.8337e-02,  3.7183e-02,  7.2420e-02,  7.9169e-02,  1.7054e-02,\n",
      "        -7.6389e-03, -2.5519e-02, -8.3059e-04, -3.7853e-03,  8.4816e-02,\n",
      "        -3.9735e-02,  1.1444e-01,  1.9909e-02,  4.5714e-02, -4.1858e-02,\n",
      "         5.2136e-02,  6.4853e-02,  3.3627e-02,  2.2914e-02,  4.5603e-03,\n",
      "        -3.9724e-02, -1.0646e-02, -3.9032e-02, -1.6484e-02, -4.2613e-02,\n",
      "        -6.1381e-03, -1.8662e-02,  1.3355e-02, -1.2730e-02,  5.1242e-03,\n",
      "        -4.9536e-02, -2.1270e-02,  1.3530e-02, -2.7411e-02, -5.1825e-03,\n",
      "         6.6643e-02,  4.1234e-02, -6.8912e-02,  5.4044e-02, -2.7545e-02,\n",
      "        -7.6215e-02,  7.1486e-03,  2.6770e-02, -6.4983e-02,  6.7193e-02,\n",
      "         3.2720e-02,  1.4127e-01, -2.0021e-02,  9.6908e-03,  5.3734e-02,\n",
      "         3.5091e-02, -5.7533e-02,  2.0720e-02, -1.4837e-02, -3.0928e-01,\n",
      "         2.9716e-02, -1.6593e-02, -4.3332e-03, -2.9584e-03,  4.1487e-02,\n",
      "         2.6126e-02, -3.1705e-02, -1.8210e-02,  3.7407e-02, -3.6394e-02,\n",
      "        -2.6218e-02, -1.1018e-01, -2.9818e-02, -4.5345e-02,  6.9618e-02,\n",
      "         4.1163e-02,  1.1645e-01, -6.8282e-02, -5.2381e-02, -4.1626e-02,\n",
      "         1.3779e-01,  2.9526e-02,  1.5967e-03,  8.6712e-02,  2.0999e-02,\n",
      "         1.4158e-01, -2.2470e-02, -8.1971e-02,  2.5568e-03,  5.8326e-03,\n",
      "         6.2637e-02,  5.9727e-02, -3.4833e-02, -1.6268e-02, -2.3802e-02,\n",
      "        -3.4639e-02,  4.4856e-02,  6.0985e-02,  4.4513e-02, -7.3297e-03,\n",
      "        -4.4620e-02,  4.2732e-04, -1.9194e-02,  4.2784e-02,  9.2780e-02,\n",
      "        -1.6814e-01, -1.6136e-02, -2.0326e-02, -6.3765e-02, -1.5375e-02,\n",
      "         4.6536e-02, -1.8560e-02, -4.0254e-02,  4.1601e-02, -2.9461e-03,\n",
      "         2.7907e-02, -1.1811e-02,  7.2378e-02, -3.0431e-02,  5.7058e-02,\n",
      "        -3.3069e-02, -7.6922e-03, -9.6532e-02, -3.9662e-02,  3.5401e-02,\n",
      "         1.8408e-01,  1.3345e-03,  3.5922e-02,  1.8300e-02, -8.8322e-02,\n",
      "         1.3748e-02, -1.3695e-01,  9.6428e-03, -2.8054e-02, -8.4108e-02,\n",
      "         3.0158e-02,  4.7794e-02, -3.9612e-02, -5.6548e-02, -1.5983e-01,\n",
      "         3.7929e-02, -3.4076e-02,  2.0406e-02,  3.9762e-02,  1.3089e-02,\n",
      "         7.5598e-03, -2.5873e-02, -3.4380e-02, -2.3249e-01,  2.5732e-02,\n",
      "        -2.3082e-02, -1.8845e-02, -3.1617e-02, -9.9163e-03, -6.5361e-04,\n",
      "         6.5202e-02,  3.2344e-02, -3.5677e-02,  5.0139e-02, -3.6806e-02,\n",
      "         9.7838e-03, -4.9867e-03, -1.0119e-01, -2.7648e-02,  2.6120e-02,\n",
      "        -3.4110e-02,  8.3972e-02,  3.3347e-02, -1.1623e-01, -1.8414e-02,\n",
      "        -5.6420e-02,  2.1816e-02,  1.1463e-01, -1.4598e-01,  4.0478e-02,\n",
      "         4.7019e-02,  4.3775e-02,  2.7642e-02,  4.0313e-02,  2.4315e-02,\n",
      "         1.4539e-01, -6.9153e-02,  9.7057e-02, -3.1922e-02,  6.3033e-02,\n",
      "        -1.8918e-02,  5.5040e-02, -6.0756e-02, -2.9630e-02,  4.3652e-03,\n",
      "         7.1152e-02, -1.0157e-01,  1.2418e-01, -1.6791e-02,  5.8537e-03,\n",
      "        -1.1520e-01, -7.4530e-03, -9.6579e-03,  2.0830e-01,  2.4564e-02,\n",
      "         6.8207e-02,  4.5188e-02, -1.2008e-02, -7.8553e-02, -1.2485e-01,\n",
      "         3.8610e-02,  1.9374e-02,  6.1652e-02, -1.4656e-02,  5.6602e-02,\n",
      "        -6.2870e-02,  1.0035e-01, -2.6944e-02,  7.3898e-02, -6.1298e-02,\n",
      "        -3.2592e-02,  3.8916e-02, -6.5290e-02,  2.4442e-02, -7.8086e-02,\n",
      "        -1.1351e-01, -2.4896e-02,  8.4275e-03,  1.6442e-02,  1.5216e-01,\n",
      "         6.3665e-03, -3.7994e-01, -3.8550e-02,  4.0892e-03, -3.7332e-02,\n",
      "         5.8010e-03, -2.2478e-02,  2.5680e-02,  1.6485e-02, -5.9719e-02,\n",
      "         4.3622e-02, -1.6667e-02, -1.6845e-02, -6.6780e-02, -4.2441e-02,\n",
      "        -3.3389e-02, -3.7480e-02, -2.5454e-01,  3.8028e-02, -5.7642e-02,\n",
      "         3.7871e-02,  6.8123e-02, -1.8972e-02, -7.8982e-03,  2.3426e-02,\n",
      "        -1.4747e-02, -1.5210e-02,  3.4643e-02, -1.1673e-02, -2.6723e-02,\n",
      "        -1.6822e-02,  5.4574e-03,  1.8909e-02,  1.6699e-01,  5.0330e-02,\n",
      "        -1.2814e-01, -2.7510e-02,  5.9903e-02, -2.3774e-02, -3.4193e-02,\n",
      "        -1.1005e-01,  4.1063e-02, -4.0414e-02, -3.4095e-02,  1.0899e-01,\n",
      "         5.9065e-02, -3.5430e-02, -3.7019e-02, -2.5776e-02,  4.0116e-02,\n",
      "         1.3292e-02, -1.5387e-02,  3.0060e-02,  1.2222e-02,  5.8398e-02,\n",
      "        -1.2132e-02,  4.2384e-03, -2.6140e-02,  5.6432e-02, -3.1099e-02,\n",
      "         6.7899e-03, -8.9754e-02, -5.2514e-02,  1.2403e-02,  1.2662e-02,\n",
      "        -7.8078e-02,  6.7368e-03,  7.5605e-02, -1.6249e-02, -5.1016e-02,\n",
      "         1.4941e-02, -2.2791e-02,  1.0511e-01, -4.2893e-02,  8.3574e-03,\n",
      "         1.8703e-03, -1.7826e-02,  1.5324e-02,  1.0722e-01,  8.2967e-04,\n",
      "        -1.1686e-01,  4.2339e-02, -1.0717e-02,  1.0533e-02,  1.1102e-01,\n",
      "         1.5918e-02, -5.6782e-04,  4.3066e-02, -5.9668e-02,  6.6311e-02,\n",
      "         2.0338e-03,  2.0829e-02, -4.7900e-02, -5.9077e-02, -2.9278e-02,\n",
      "        -2.4974e-02,  5.1157e-02, -6.1032e-02, -3.5734e-03,  2.3759e-02,\n",
      "         6.2802e-02,  5.1943e-04,  3.9025e-02,  7.1760e-02,  4.1675e-02,\n",
      "        -2.6994e-02,  5.4310e-02, -1.1072e-02, -9.9313e-03,  1.9879e-02,\n",
      "         5.9876e-02,  2.9036e-02,  3.0424e-03, -5.4386e-02, -9.7087e-02,\n",
      "        -6.5499e-03,  9.7680e-02, -4.8947e-02, -1.7396e-02,  1.6917e-02,\n",
      "        -5.2530e-02, -1.7217e-02, -4.7341e-02, -3.4153e-02, -4.5355e-02,\n",
      "        -8.4201e-03, -4.1909e-03, -2.0398e-02,  8.7812e-02,  1.8465e-02,\n",
      "         9.7138e-02, -1.4636e-02, -8.5972e-02, -6.6289e-02, -1.2861e-02,\n",
      "        -7.2674e-02, -6.6145e-02,  4.4596e-02,  1.2476e-01, -2.4054e-02,\n",
      "         6.5817e-02, -3.4166e-03, -4.9155e-02,  2.2870e-02, -2.9501e-02,\n",
      "        -1.0209e-01,  1.5357e-01, -2.2418e-02,  1.1786e-02,  3.2676e-02,\n",
      "        -4.8057e-02, -4.8576e-02,  6.6755e-02,  6.2084e-02,  8.7952e-02,\n",
      "        -2.9730e-02,  1.9486e-02,  5.5622e-02, -4.7538e-02,  3.0426e-01,\n",
      "        -4.4062e-02,  8.0127e-03, -4.5327e-02,  1.5974e-02,  1.3313e-01,\n",
      "        -8.3618e-02,  2.6789e-01,  2.0256e-04, -2.6752e-02, -1.3353e-01,\n",
      "        -5.6864e-02, -5.7893e-02, -2.3041e-02,  6.6110e-03,  1.3170e-02,\n",
      "         7.2117e-02,  3.1041e-02, -7.0947e-02, -2.2347e-02,  2.9180e-02,\n",
      "        -1.1808e-01, -5.5194e-02,  7.0301e-02,  1.3799e-03,  1.4448e-03,\n",
      "         2.0458e-02, -3.0997e-02,  3.0163e-02,  4.2879e-02, -3.0777e-02,\n",
      "        -3.2440e-02, -4.6119e-02, -1.3583e-02,  5.2260e-02,  1.3423e-02,\n",
      "         1.5146e-02, -4.0836e-02,  5.8244e-02, -5.4241e-02,  2.4858e-02,\n",
      "         5.0231e-02,  4.0738e-02,  1.4036e-02, -4.8872e-02, -1.5233e-02,\n",
      "        -2.6912e-02,  3.8677e-02,  5.4268e-02, -8.0999e-03, -3.9796e-02,\n",
      "         9.6665e-02, -4.7765e-03,  5.3869e-02,  3.3038e-02, -2.3547e-02,\n",
      "         2.4535e-02, -3.5074e-02,  4.9867e-02, -4.3393e-02,  2.9646e-03,\n",
      "         2.2689e-03,  7.4725e-02,  1.2125e-02,  5.5846e-03, -3.6314e-02,\n",
      "         6.3971e-02,  6.6495e-03,  9.9586e-02,  9.7931e-02, -1.1987e-01,\n",
      "        -3.2080e-03, -5.7377e-02,  9.7054e-02,  6.0844e-02, -3.3348e-02,\n",
      "         4.3876e-02,  1.3905e-02, -6.2332e-03, -2.6113e-02, -7.7367e-04,\n",
      "         1.5743e-01, -4.8908e-02, -2.1986e-02, -5.9246e-02,  1.9880e-02,\n",
      "        -4.0352e-02, -8.4162e-03, -6.5323e-02, -3.8609e-02,  2.4498e-02,\n",
      "         4.5310e-02, -3.6717e-02,  5.4876e-02, -6.0021e-02,  1.0822e-03,\n",
      "        -5.2042e-03, -4.4298e-02,  3.5101e-02,  5.9192e-02, -8.1531e-02,\n",
      "         1.5138e-02,  8.2949e-03, -1.7302e-02,  3.7347e-02,  3.3272e-02,\n",
      "        -4.5675e-02,  6.7666e-02,  4.2874e-02,  4.2704e-03,  1.8794e-02,\n",
      "         4.4080e-02,  3.8399e-02,  1.2875e-02, -7.9689e-03,  4.4803e-02,\n",
      "         3.3418e-02, -5.1692e-02, -1.1462e-01, -4.3953e-02,  6.1990e-02,\n",
      "        -5.4157e-03,  5.8201e-02, -3.7194e-03,  5.0561e-02,  4.0040e-02,\n",
      "        -1.8059e-01,  1.1111e-02, -1.8355e-02, -7.4158e-02, -5.0034e-02,\n",
      "        -2.6872e-02, -7.5339e-02, -9.6043e-02,  1.5408e-02, -3.1506e-03,\n",
      "        -2.1035e-01, -6.0785e-02,  2.7664e-02, -6.4732e-02, -9.4740e-03,\n",
      "         1.6365e-01, -2.4740e-02, -9.8945e-04, -4.8277e-02, -2.3016e-02,\n",
      "         2.6772e-02, -4.6622e-02, -3.5961e-02,  8.4219e-03, -2.7068e-02,\n",
      "        -3.4036e-02,  5.2317e-02,  3.3845e-02, -1.0434e-01,  3.6302e-02,\n",
      "         3.6434e-02, -3.1725e-02, -3.3709e-03,  9.9479e-02,  8.9896e-02,\n",
      "        -2.5820e-02, -3.8016e-02,  7.3462e-02,  6.2386e-02, -5.7812e-03,\n",
      "         7.1372e-02, -8.7203e-03,  2.5187e-02, -2.1683e-02,  7.7098e-02,\n",
      "         7.5658e-02,  5.5210e-02, -2.0943e-02, -3.4944e-02,  7.4399e-02,\n",
      "         7.8916e-02,  1.6574e-02, -1.7016e-02,  7.2884e-02,  2.0227e-02,\n",
      "        -1.6340e-02, -4.3448e-02,  7.8566e-03,  6.2620e-03, -5.7073e-02,\n",
      "        -2.9198e-02, -5.3091e-04, -4.2376e-02,  3.7240e-02, -1.6885e-02,\n",
      "        -1.2701e-02,  4.3958e-02, -6.7571e-03, -1.3108e-03,  2.6123e-01,\n",
      "         6.5451e-02, -2.6455e-02, -4.4731e-02, -4.9834e-02,  6.5798e-02,\n",
      "         2.9217e-02,  3.9079e-02,  3.7441e-02, -5.5701e-02, -6.4402e-02,\n",
      "         6.5085e-02,  5.9396e-02, -4.0397e-02,  1.5574e-02, -7.0103e-02,\n",
      "        -4.3423e-02,  4.8691e-03, -2.9594e-02,  5.5141e-02,  2.5787e-02,\n",
      "        -3.8881e-02,  1.4910e-01,  1.1949e-01, -1.2815e-01,  3.5506e-02,\n",
      "         1.3216e-01,  5.5624e-02, -5.0694e-02,  6.9278e-02, -3.9865e-03,\n",
      "        -1.8093e-03,  3.0629e-02,  2.3523e-02, -4.9062e-02, -2.5185e-02,\n",
      "         6.4341e-03,  1.6671e-02, -7.9994e-02, -2.6383e-02, -2.0879e-01,\n",
      "        -2.9898e-02, -6.9908e-02,  1.2029e-02,  4.2714e-02, -4.7473e-02,\n",
      "         2.1429e-03, -3.6004e-02, -4.2082e-02,  4.2812e-02, -6.6419e-02,\n",
      "         1.0997e-01, -6.5729e-02, -4.9841e-02,  1.6656e-02,  3.1415e-02,\n",
      "        -5.7441e-02, -6.0160e-02, -7.2954e-02,  7.1149e-02, -1.4271e-02,\n",
      "         4.5810e-02,  5.3830e-02,  2.1894e-02,  1.6546e-01,  4.5697e-02,\n",
      "         2.6916e-02, -2.7048e-02, -1.9920e-02,  1.0500e-01,  2.0120e-02,\n",
      "         2.7550e-02, -1.2268e-02, -8.0973e-03,  2.5751e-02, -1.8949e-02,\n",
      "         1.5144e-02, -2.4832e-02, -8.3692e-03,  1.5290e-02,  1.3617e-01,\n",
      "        -6.1772e-02, -5.9527e-02,  6.2119e-03, -5.9973e-03, -9.0072e-03,\n",
      "         1.2622e-02,  5.7083e-02, -3.7398e-02,  4.0929e-02, -2.2365e-02,\n",
      "        -4.5087e-02, -3.0320e-02, -1.2487e-02, -2.7840e-02,  4.2891e-02,\n",
      "        -7.9183e-02, -3.5572e-02,  3.1568e-02, -3.1701e-02, -4.3825e-02,\n",
      "         6.4583e-02,  5.9620e-02,  2.1913e-01, -1.4988e-03, -1.9471e-02,\n",
      "         1.9220e-02, -7.6000e-02, -5.5282e-03, -1.0387e-02, -1.0189e-02,\n",
      "         6.3849e-03, -2.8259e-03, -1.6618e-02,  3.8341e-02,  4.4083e-02,\n",
      "        -1.5755e-02, -9.0587e-02, -3.1568e-02, -5.3352e-02,  4.7646e-02,\n",
      "         2.3109e-02,  6.4521e-02, -4.3378e-02, -3.0408e-02,  3.2752e-02,\n",
      "         2.1714e-02,  1.0364e-02, -6.4582e-02, -2.1034e-02,  2.0546e-02,\n",
      "        -4.9449e-02,  2.0778e-02,  2.5967e-02,  1.7578e-02,  8.6467e-03,\n",
      "         4.3441e-02,  4.1620e-02,  1.8622e-01,  1.3159e-01, -3.9374e-02,\n",
      "         1.2458e-01,  1.3856e-01,  3.6255e-02,  4.2359e-02, -9.6070e-02,\n",
      "        -3.9130e-02, -2.9944e-02,  1.4274e-02, -3.8774e-02, -5.6708e-02,\n",
      "        -1.2649e-02,  6.9196e-02,  1.8652e-02,  3.2774e-02,  1.6889e-01,\n",
      "        -2.2597e-01,  2.1810e-03,  9.1879e-02, -5.9082e-02,  4.1439e-02,\n",
      "         1.0167e-02,  4.7797e-02, -6.2403e-02, -5.2983e-02, -6.4438e-02,\n",
      "         9.8941e-03, -1.7169e-02, -5.4309e-02, -6.0473e-03,  3.1492e-02,\n",
      "        -3.7452e-02,  5.7129e-02, -5.4266e-03,  4.5567e-02,  2.1004e-02,\n",
      "         3.3298e-02,  7.3744e-02, -2.9590e-02,  3.3138e-02, -1.4042e-02,\n",
      "        -2.3311e-02, -5.2958e-03,  1.1469e-01, -2.3026e-03,  4.8549e-02,\n",
      "        -2.8003e-02, -5.6171e-02, -3.8028e-02,  5.0102e-02, -4.6255e-02,\n",
      "         2.8888e-02,  6.9829e-02, -2.4590e-02, -2.6569e-02,  1.0669e-02,\n",
      "         6.1487e-02, -3.2991e-02,  1.2918e-01, -1.3167e-01, -6.9717e-02,\n",
      "         3.1807e-02, -7.1666e-02, -1.8436e-03, -7.3102e-02,  3.0629e-02,\n",
      "        -6.0614e-02, -9.1447e-02, -2.5511e-02,  3.7861e-02,  1.9850e-02,\n",
      "         2.1382e-02,  6.2519e-02, -9.8499e-03, -3.4642e-02, -5.5468e-02,\n",
      "        -7.0303e-03,  7.3770e-02, -1.4056e-01, -2.3865e-02, -7.8753e-02,\n",
      "         1.2013e-02,  5.6952e-02, -4.8979e-02, -4.3937e-03,  2.8473e-02,\n",
      "         3.5404e-02,  8.1924e-02, -2.7104e-02,  2.8799e-02, -7.7909e-03,\n",
      "         1.1213e-01, -3.5446e-02, -2.5554e-02,  2.2784e-02,  1.2636e-02,\n",
      "         2.0957e-02,  7.7390e-02,  1.7789e-02, -4.6335e-04,  1.2183e-02,\n",
      "         6.3358e-03,  1.0940e-02, -1.5470e-02, -3.6520e-02,  4.7757e-02,\n",
      "        -2.3672e-02, -3.3552e-02,  1.6054e-02,  1.2696e-01,  2.4827e-02,\n",
      "        -5.8589e-03, -1.6736e-02,  3.2125e-02, -2.2988e-03,  7.3065e-02,\n",
      "        -2.2709e-02,  3.8252e-02, -8.0299e-02, -9.7390e-03, -2.2015e-01,\n",
      "        -2.5009e-01, -4.8274e-03,  1.5082e-01, -2.3302e-02, -1.7456e-01,\n",
      "        -1.1340e-02, -6.1596e-02, -2.8987e-02, -2.0049e-02, -1.5173e-01,\n",
      "        -5.6023e-02, -3.6708e-02,  3.6052e-02,  1.6582e-02,  2.2738e-02,\n",
      "         1.8063e-02, -1.1532e-01,  2.8268e-02,  7.3795e-02,  9.0827e-02,\n",
      "        -3.6785e-02,  7.7644e-02,  5.5304e-02,  6.4842e-02,  8.1224e-02,\n",
      "        -6.4783e-02, -5.3624e-02, -6.1587e-02,  5.9450e-02, -7.0698e-02,\n",
      "        -1.4222e-02, -5.1231e-02, -8.4451e-02,  1.8375e-02,  5.5298e-02,\n",
      "         7.1843e-02,  1.8660e-02,  1.6023e-02, -3.6735e-02,  3.1865e-02,\n",
      "        -3.6583e-02,  1.5578e-01,  9.6505e-03,  9.5462e-02, -1.1732e-01,\n",
      "        -2.6245e-02,  2.4857e-02,  4.0002e-02, -7.0224e-02,  1.6450e-02,\n",
      "         2.6372e-02,  1.3022e-02, -4.2807e-02, -8.0039e-02, -5.3939e-03,\n",
      "        -2.1590e-02,  6.3614e-02,  1.0206e-01,  1.2048e-01,  2.2036e-02,\n",
      "        -1.2808e-01,  1.2326e-02, -7.6501e-02, -3.5547e-02, -2.9704e-02,\n",
      "         9.5050e-02,  2.7388e-02, -1.8956e-01,  6.6547e-02, -9.8203e-02,\n",
      "        -2.8378e-02,  6.7356e-02,  1.6263e-02,  3.8478e-02,  3.0266e-02,\n",
      "        -6.9614e-02,  4.2905e-02,  2.7853e-02, -4.4865e-02,  1.3796e-02,\n",
      "        -3.4994e-03, -3.8252e-03,  5.2731e-02,  2.3616e-02, -1.3824e-02,\n",
      "        -4.3851e-02, -3.0300e-02,  3.0612e-02, -7.4535e-02,  2.0063e-02,\n",
      "         1.9931e-01, -7.3473e-03,  5.1847e-02, -1.2029e-01,  4.8646e-02,\n",
      "         7.8768e-02, -2.4242e-02,  4.8029e-02, -1.5302e-02,  8.0289e-02,\n",
      "         1.5506e-02, -1.4114e-02,  2.6519e-02, -2.0672e-02, -1.1581e-01,\n",
      "        -3.5250e-02, -6.5159e-02, -4.8953e-02, -2.9985e-02,  1.5427e-02,\n",
      "         6.9451e-02,  5.4109e-02, -5.0889e-02, -1.6022e-02], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "params = model.state_dict()\n",
    "embeddings = params['transformer.wte.weight']\n",
    "embeddings = embeddings[50257:]\n",
    "torch.set_printoptions(threshold=10000)\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Children's Book Test Dataset\n",
    "Also taken from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "cbt_df = pd.read_csv(\"/home/ubuntu/test/cbt/cbt_extract/cbt_csv_files/updated_cbt_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on One Example\n",
    "Writing a function that is reusable and works for one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cbt(example):\n",
    "    # extract the sentence\n",
    "    sentence = example[\"sentences\"]\n",
    "    sentence = sentence.replace('\"','')\n",
    "    sentence = sentence.replace(\"\\n\",\"\")\n",
    "    sentence = sentence[1:len(sentence)-1]\n",
    "\n",
    "    # extract the question\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    # make the total query with the sentence and the question\n",
    "    total_query = sentence + \" \" + question\n",
    "\n",
    "    print(total_query)\n",
    "\n",
    "    \n",
    "    #replace words in example with winodict fake words (currently, we replace all possible identified examples)\n",
    "    replace_words = example[\"winodict_words\"]\n",
    "    replace_words = replace_words[1:len(replace_words)-1].replace(\"'\",\"\").split(\",\")\n",
    "\n",
    "    for word in replace_words:\n",
    "        word = word.strip()\n",
    "        word = \" \"+word+\" \"\n",
    "        while word in total_query:\n",
    "            total_query = total_query.replace(word,\" \"+real_to_fake_dict[word.strip()]+\" \")\n",
    "    \n",
    "\n",
    "    # get the answer options for the model into a list\n",
    "    options = example[\"options\"].replace(\"'\",\"\")\n",
    "    options = options.replace(\"\\n\",\"\")\n",
    "    options = options[1:len(options)-1].split(\" \")\n",
    "\n",
    "    # get the answer to the problem\n",
    "    answer = example[\"answer\"]\n",
    "    print(answer)\n",
    "    print(options)\n",
    "\n",
    "    \n",
    "    if answer in replace_words:\n",
    "        answer = real_to_fake_dict[answer][0]\n",
    "    \n",
    "    \n",
    "\n",
    "    # initialize best answer and best loss (will be lowest value)\n",
    "    best_answer = \"\"\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    # compute the loss for each option (using full scoring)\n",
    "    for option in options:\n",
    "        \n",
    "        if option in replace_words:\n",
    "            option = real_to_fake_dict[option][0]\n",
    "        \n",
    "\n",
    "        updated_query = total_query.replace(\"XXXXX\", option)\n",
    "        \n",
    "        # Tokenize each string and produce labels\n",
    "        updated_input = tokenizer(updated_query, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        \"\"\"\n",
    "        if len(updated_input[0]) > 1024:\n",
    "            return -1\n",
    "        \"\"\"\n",
    "\n",
    "        #print(updated_input['input_ids'])\n",
    "\n",
    "        current_loss = model(**updated_input, labels=updated_input[\"input_ids\"].to(\"cuda\")).loss\n",
    "        \n",
    "        if current_loss < best_loss:\n",
    "            best_answer = option\n",
    "            best_loss = current_loss\n",
    "    return best_answer == answer \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating CBT on GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_indices = []\n",
    "\n",
    "with open(\"/home/ubuntu/test/cbt/cbt_eval_info/eval_indices\", \"r\") as f:\n",
    "    eval_indices = f.read()\n",
    "\n",
    "eval_indices = eval_indices[1:len(eval_indices)-1].split(\", \")\n",
    "for i in range(len(eval_indices)):\n",
    "    eval_indices[i] = int(eval_indices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'You must have noticed it , Jaqueline ; you sat up later .' How the dogs howled ! '' `` No ; I mean yes , '' murmured poor Jaqueline , who of course had caused the whole affair by her magic arts , but who had forgotten , in the excitement of the moment , that an eclipse of the moon , especially if entirely unexpected , is likely to attract very general attention . 'Jaqueline could not bear to tell a fib , especially to a king who had been so kind to her ; besides , fibbing would not alter the facts .' `` Yes , I did see it , '' she admitted , blushing . `` Had it not been predicted ? '' `` Not a word about it whispered anywhere , '' said his Majesty . '`` I looked up the almanack at once .' It is the most extraordinary thing I ever saw , and I 've seen a good many . '' `` The astronomers must be duffers , '' said Prince Ricardo . '`` I never thought there was much in physical science of any sort ; most dreary stuff .' Why , they say the earth goes round the sun , whereas any fool can see it is just the other way on . '' 'King Prigio was struck aghast by these sentiments in the mouth of his son and heir , the hope of Pantouflia .' 'But what was the king to say in reply ?' 'The astronomers of Pantouflia , who conceived that they knew a great deal , had certainly been taken by surprise this time .' 'Indeed , they have not yet satisfactorily explained this eclipse of the moon , though they have written volumes about it .' `` Why , it may be the sun next ! '' 'exclaimed his Majesty .' '`` Anything may happen .' The very laws of gravitation themselves may go askew ! '' At this moment the butler , William , who had been in the queen 's family when she was a girl , entered , and announced : `` Some of the royal tradesmen , XXXXX appointment , to see your Majesty . ''\n",
      "by\n",
      "['about', 'at', 'by', 'if', 'in', 'of', 'on', 'that', 'to', 'whereas']\n",
      "1\n",
      "\n",
      "Number correct:  1\n",
      "Total:  1\n",
      "Correct percentage:  1.0\n"
     ]
    }
   ],
   "source": [
    "# evaluating gpt-2 AFTER REPLACING words and saving indices and stats\n",
    "correct_list = []\n",
    "total_list = []\n",
    "\n",
    "correct = 0 \n",
    "total = 0 \n",
    "for index, row in cbt_df.iterrows():\n",
    "    #print(index)\n",
    "\n",
    "    if index not in eval_indices:\n",
    "        continue\n",
    "\n",
    "    update = evaluate_cbt(row)\n",
    "    if update == -1:\n",
    "        continue\n",
    "    if update == 1:\n",
    "        correct_list.append(index)\n",
    "    total_list.append(index)\n",
    "    correct += update\n",
    "    total += 1\n",
    "    print(total)\n",
    "    if total == 600:\n",
    "        break\n",
    "    if total == 200:\n",
    "        break\n",
    "    break\n",
    "    print(\"\")\n",
    "\n",
    "correct_list = str(correct_list)\n",
    "total_list = str(total_list)\n",
    "\n",
    "\n",
    "with open(\"/home/ubuntu/test/cbt/cbt_eval_info/GPT2-CLSTokenOnly/GPT2-CLSTokenOnly_correct\", \"w\") as file1:\n",
    "    file1.write(correct_list)\n",
    "\n",
    "with open(\"/home/ubuntu/test/cbt/cbt_eval_info/GPT2-CLSTokenOnly/GPT2-CLSTokenOnly_total\", \"w\") as file2:\n",
    "    file2.write(total_list)\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Number correct: \", correct)\n",
    "print(\"Total: \", total)\n",
    "print(\"Correct percentage: \",correct/total)\n",
    "\n",
    "information_list = str([correct, total, correct/total])\n",
    "\n",
    "\n",
    "with open(\"/home/ubuntu/test/cbt/cbt_eval_info/GPT2-CLSTokenOnly/GPT2-CLSTokenOnly_statistics\", \"w\") as file3:\n",
    "    file3.write(information_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
