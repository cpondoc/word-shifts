{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS 224N - Finetuning on Example Sentences\n",
    "Finetune on the downstream task of sample sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using GPU: \" + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all WinoDict Words\n",
    "Get all the words from the WinoDict dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "first_set = pd.read_csv(\"winodict/prob1_of_5.csv\")\n",
    "real_words, winodict_words = first_set['lemma'].tolist(), first_set['fake_lemma'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Get WordNet Definition of WinoDict Word\n",
    "Using WordNet to get the WordNet definition of a fake word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def find_definition(word):\n",
    "    definition = \"\"\n",
    "    for synset in wn.synsets(word):\n",
    "        definition += synset.definition() + \". \"\n",
    "    return definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in G2G/R2G Model for Predicting Embeddings\n",
    "This will be used to initialize our initial word embeddings for the fake words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# G2G/R2G used to Predict\n",
    "predict_model = GPT2LMHeadModel.from_pretrained(\"weights/G2GNext1\").to(\"cuda\")\n",
    "predict_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "predict_tokenizer.add_tokens(['[CLS]'])\n",
    "\n",
    "# GPT-2 Model and Tokenizer to be fine-tuned\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium').to(\"cuda\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "\n",
    "# Standalone tokenizer\n",
    "ORIG_TOKENIZER = GPT2Tokenizer.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 1:1 Fake Word to Real Word Ratio\n",
    "Make sure there is established parity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set for all real and fake words seen\n",
    "real_words_seen = set()\n",
    "fake_words_seen = set()\n",
    "\n",
    "# Final dataset -- should have 1:1 for each\n",
    "final_real_words = []\n",
    "final_fake_words = []\n",
    "\n",
    "# Iterate through all combinations\n",
    "for i in range(len(real_words)):\n",
    "    # Case 1: not already seen\n",
    "    if real_words[i] not in real_words_seen and winodict_words[i] not in fake_words_seen:\n",
    "        # Add to seen sets\n",
    "        fake_words_seen.add(winodict_words[i])\n",
    "        real_words_seen.add(real_words[i])\n",
    "        \n",
    "        # Add to arrays\n",
    "        final_real_words.append(real_words[i])\n",
    "        final_fake_words.append(winodict_words[i])\n",
    "    \n",
    "    # Case 2: real word has not been seen, fake word has\n",
    "    elif real_words[i] not in real_words_seen and winodict_words[i] in fake_words_seen:\n",
    "        # Modify until new word in vocabulary\n",
    "        curr_word = winodict_words[i]\n",
    "        while (curr_word in fake_words_seen):\n",
    "            curr_word += \"z\"\n",
    "        \n",
    "        # Add to seen sets\n",
    "        fake_words_seen.add(curr_word)\n",
    "        real_words_seen.add(real_words[i])\n",
    "        \n",
    "        # Add to arrays\n",
    "        final_real_words.append(real_words[i])\n",
    "        final_fake_words.append(curr_word)\n",
    "\n",
    "# Quick sanity check\n",
    "assert(len(final_real_words) == len(final_fake_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Real and Fake Word Pairings\n",
    "Take the two arrays and save them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('datasets/realtofake.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    writer.writerow([\"Real\", \"Fake\"])\n",
    "    for i in range(len(final_real_words)):\n",
    "        writer.writerow([final_real_words[i], final_fake_words[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect All Definitions for the 1:1 Dataset\n",
    "Gather all of the definitions together for each of the real words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all of the WordNet definitions\n",
    "final_definitions = []\n",
    "for real_word in final_real_words:\n",
    "    final_definitions.append(find_definition(real_word))\n",
    "\n",
    "# Quick sanity check\n",
    "assert(len(final_definitions) == len(final_fake_words))\n",
    "assert(len(final_definitions) == len(final_real_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Add All Embeddings\n",
    "For each of the corresponding real and fake words, plus definitions, create an embedding using G2G/R2G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total definitions: 343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helpful Debug Message\n",
    "print(\"Number of total definitions: \" + str(len(final_definitions)))\n",
    "\n",
    "# Tokenizing all of the definitions at once\n",
    "predict_tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_inputs = predict_tokenizer(final_definitions, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=511)\n",
    "tokenized_cls = predict_tokenizer([\" [CLS]\"] * len(final_definitions), return_tensors=\"pt\")\n",
    "tokenized_inputs['input_ids'] = torch.cat((tokenized_inputs['input_ids'], tokenized_cls['input_ids']), dim=1).to(\"cuda\")\n",
    "\n",
    "# Add the new tokens and resize the model embeddings matrix\n",
    "displacement = len(tokenizer)\n",
    "tokenizer.add_tokens(final_fake_words)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "params = model.state_dict()\n",
    "\n",
    "# Adding new embeddings in a range of 4\n",
    "for i in range(0, len(final_definitions), 4):\n",
    "    outputs = predict_model(input_ids=tokenized_inputs['input_ids'][i:min(len(final_definitions), i + 4)], output_hidden_states=True)\n",
    "    params['transformer.wte.weight'][displacement + i: displacement + min(len(final_definitions), i + 4),:] = outputs.hidden_states[-1][:,511,:].detach().clone()\n",
    "model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check for Maintaining Index Parity\n",
    "Make sure that all of the embeddings, definitions, and fake words are in the same place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prodittionzzz\n",
      "a boxlike container in a piece of furniture; made so as to slide in and out. the person who writes a check or draft instructing the drawee to pay someone else. an artist skilled at drawing. \n",
      "{'input_ids': tensor([[50298]]), 'attention_mask': tensor([[1]])}\n",
      "prodittionzzz\n",
      "50298\n",
      "nturyzzz\n",
      "having or showing feelings of unwarranted importance out of overbearing pride. \n",
      "{'input_ids': tensor([[50599]]), 'attention_mask': tensor([[1]])}\n",
      "nturyzzz\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check #1: at index 41\n",
    "print(final_fake_words[41])\n",
    "print(final_definitions[41])\n",
    "print(tokenizer(\"prodittionzzz\", return_tensors=\"pt\"))\n",
    "print(tokenizer.decode(displacement + 41))\n",
    "print(displacement + 41)\n",
    "\n",
    "# Sanity Check #1: at the last index\n",
    "print(final_fake_words[-1])\n",
    "print(final_definitions[-1])\n",
    "print(tokenizer(\"nturyzzz\", return_tensors=\"pt\"))\n",
    "print(tokenizer.decode(displacement + len(final_fake_words) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure Correct Tokenization of Sample Sentences\n",
    "Make sure all of the sample sentences are correctly tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load in all of the example sentences\n",
    "example_sentences = json.load(open('datasets/example_sentences.json'))\n",
    "\n",
    "# Collecting all of the sentences and words used for training\n",
    "fake_word_to_sentences = defaultdict(list)\n",
    "\n",
    "# Iterate through each example sentence\n",
    "for word in example_sentences:\n",
    "    for sentence in example_sentences[word]:\n",
    "        # Get the corresponding fake word and craft the new sentence\n",
    "        fake_word = final_fake_words[final_real_words.index(word)]\n",
    "        fake_sentence = sentence.replace(word, fake_word) \n",
    "        \n",
    "        # Check if tokenized correctly\n",
    "        tokens = tokenizer(fake_sentence)['input_ids']\n",
    "        word_token = tokenizer(fake_word)['input_ids'][0]\n",
    "        if (word_token in tokens):\n",
    "            fake_word_to_sentences[fake_word].append(fake_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class for Example Sentences\n",
    "Key: makes sure that the training sentences are already a tensor of the tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ExampleSentencesData(Dataset):\n",
    "\n",
    "    def __init__(self, training_sentences, tokenizer):\n",
    "        self.training_sentences = tokenizer(training_sentences, return_tensors=\"pt\", padding='max_length', truncation=True)['input_ids'].to(\"cuda\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sentence': self.training_sentences[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Creating the training loader and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                    | 1/303 [00:11<58:11, 11.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Get the tokens for each of the words in the fake sentence\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#sentence_tokens = tokenizer(fake_word_to_sentences[key], return_tensors=\"pt\", padding='max_length', truncation=True)['input_ids']\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#cuda_sentence_tokens = sentence_tokens.to('cuda')\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Run it through the actual model and calculate the loss\u001b[39;00m\n\u001b[1;32m     40\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m], labels\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 41\u001b[0m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Zero out gradient layers\u001b[39;00m\n\u001b[1;32m     44\u001b[0m embedding_mask[tokenizer(key)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reset the padding token and set up parameters for the training loader\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set up Adam optimizer and parameters for training loader\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train_params = {\n",
    "    'batch_size': 2,        \n",
    "    'shuffle': False,        \n",
    "    'num_workers': 0\n",
    "}\n",
    "\n",
    "# Zero out non embedding layers by setting require_grad = false\n",
    "params = model.state_dict()\n",
    "for param in params:\n",
    "    if (param != \"transformer.wte.weight\"):\n",
    "        params[param].requires_grad = False\n",
    "model.load_state_dict(params)\n",
    "\n",
    "# Iterate through each of the real words\n",
    "embedding_mask = torch.zeros(model.transformer.wte.weight.shape).to(\"cuda\")\n",
    "for key in tqdm(fake_word_to_sentences):\n",
    "    # Set up our dataset and our training oader\n",
    "    special_word_dataset = ExampleSentencesData(fake_word_to_sentences[key], tokenizer)\n",
    "    training_loader = DataLoader(special_word_dataset, **train_params)\n",
    "    \n",
    "    # Enumerate all examples in our training loader\n",
    "    for _ in range(2):\n",
    "        for j, data in enumerate(training_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get the tokens for each of the words in the fake sentence\n",
    "            #sentence_tokens = tokenizer(fake_word_to_sentences[key], return_tensors=\"pt\", padding='max_length', truncation=True)['input_ids']\n",
    "            #cuda_sentence_tokens = sentence_tokens.to('cuda')\n",
    "            \n",
    "            # Run it through the actual model and calculate the loss\n",
    "            outputs = model(input_ids=data['sentence'], labels=data['sentence'])\n",
    "            outputs.loss.backward()\n",
    "            \n",
    "            # Zero out gradient layers\n",
    "            embedding_mask[tokenizer(key)['input_ids'][0]] = 1\n",
    "            model.transformer.wte.weight.grad = model.transformer.wte.weight.grad * embedding_mask\n",
    "            embedding_mask[tokenizer(key)['input_ids'][0]] = 0\n",
    "            \n",
    "            # Take a step with the optimizer\n",
    "            optimizer.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('weights/G2G-Finetuned-2-Epochs-T/tokenizer_config.json',\n",
       " 'weights/G2G-Finetuned-2-Epochs-T/special_tokens_map.json',\n",
       " 'weights/G2G-Finetuned-2-Epochs-T/vocab.json',\n",
       " 'weights/G2G-Finetuned-2-Epochs-T/merges.txt',\n",
       " 'weights/G2G-Finetuned-2-Epochs-T/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('weights/G2G-Finetuned-2-Epochs')\n",
    "tokenizer.save_pretrained('weights/G2G-Finetuned-2-Epochs-T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(99999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
