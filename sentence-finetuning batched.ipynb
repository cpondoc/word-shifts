{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS 224N - Finetuning on Example Sentences\n",
    "Finetune on the downstream task of sample sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using GPU: \" + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all WinoDict Words\n",
    "Get all the words from the WinoDict dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "first_set = pd.read_csv(\"winodict/prob1_of_5.csv\")\n",
    "real_words, winodict_words = first_set['lemma'].tolist(), first_set['fake_lemma'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Get WordNet Definition of WinoDict Word\n",
    "Using WordNet to get the WordNet definition of a fake word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def find_definition(word):\n",
    "    definition = \"\"\n",
    "    for synset in wn.synsets(word):\n",
    "        definition += synset.definition() + \". \"\n",
    "    return definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in G2G/R2G Model for Predicting Embeddings\n",
    "This will be used to initialize our initial word embeddings for the fake words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# G2G/R2G used to Predict\n",
    "predict_model = GPT2LMHeadModel.from_pretrained(\"weights/G2GNext1\").to(\"cuda\")\n",
    "predict_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "predict_tokenizer.add_tokens(['[CLS]'])\n",
    "\n",
    "# GPT-2 Model and Tokenizer to be fine-tuned\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium').to(\"cuda\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "\n",
    "# Standalone tokenizer\n",
    "ORIG_TOKENIZER = GPT2Tokenizer.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 1:1 Fake Word to Real Word Ratio\n",
    "Make sure there is established parity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set for all real and fake words seen\n",
    "real_words_seen = set()\n",
    "fake_words_seen = set()\n",
    "\n",
    "# Final dataset -- should have 1:1 for each\n",
    "final_real_words = []\n",
    "final_fake_words = []\n",
    "\n",
    "# Iterate through all combinations\n",
    "for i in range(len(real_words)):\n",
    "    # Case 1: not already seen\n",
    "    if real_words[i] not in real_words_seen and winodict_words[i] not in fake_words_seen:\n",
    "        # Add to seen sets\n",
    "        fake_words_seen.add(winodict_words[i])\n",
    "        real_words_seen.add(real_words[i])\n",
    "        \n",
    "        # Add to arrays\n",
    "        final_real_words.append(real_words[i])\n",
    "        final_fake_words.append(winodict_words[i])\n",
    "    \n",
    "    # Case 2: real word has not been seen, fake word has\n",
    "    elif real_words[i] not in real_words_seen and winodict_words[i] in fake_words_seen:\n",
    "        # Modify until new word in vocabulary\n",
    "        curr_word = winodict_words[i]\n",
    "        while (curr_word in fake_words_seen):\n",
    "            curr_word += \"z\"\n",
    "        \n",
    "        # Add to seen sets\n",
    "        fake_words_seen.add(curr_word)\n",
    "        real_words_seen.add(real_words[i])\n",
    "        \n",
    "        # Add to arrays\n",
    "        final_real_words.append(real_words[i])\n",
    "        final_fake_words.append(curr_word)\n",
    "\n",
    "# Quick sanity check\n",
    "assert(len(final_real_words) == len(final_fake_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Real and Fake Word Pairings\n",
    "Take the two arrays and save them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('datasets/realtofake.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    writer.writerow([\"Real\", \"Fake\"])\n",
    "    for i in range(len(final_real_words)):\n",
    "        writer.writerow([final_real_words[i], final_fake_words[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect All Definitions for the 1:1 Dataset\n",
    "Gather all of the definitions together for each of the real words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all of the WordNet definitions\n",
    "final_definitions = []\n",
    "for real_word in final_real_words:\n",
    "    final_definitions.append(find_definition(real_word))\n",
    "\n",
    "# Quick sanity check\n",
    "assert(len(final_definitions) == len(final_fake_words))\n",
    "assert(len(final_definitions) == len(final_real_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G2G: Batch Add All Embeddings\n",
    "Create updated embeddings for GPT-2 using G2G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g2g_embeddings():\n",
    "    # Helpful Debug Message\n",
    "    print(\"Number of total definitions: \" + str(len(final_definitions)))\n",
    "\n",
    "    # Tokenizing all of the definitions at once\n",
    "    predict_tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = predict_tokenizer(final_definitions, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=511)\n",
    "    tokenized_cls = predict_tokenizer([\" [CLS]\"] * len(final_definitions), return_tensors=\"pt\")\n",
    "    tokenized_inputs['input_ids'] = torch.cat((tokenized_inputs['input_ids'], tokenized_cls['input_ids']), dim=1).to(\"cuda\")\n",
    "\n",
    "    # Add the new tokens and resize the model embeddings matrix\n",
    "    displacement = len(tokenizer)\n",
    "    tokenizer.add_tokens(final_fake_words)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    params = model.state_dict()\n",
    "\n",
    "    # Adding new embeddings in a range of 4\n",
    "    for i in range(0, len(final_definitions), 4):\n",
    "        outputs = predict_model(input_ids=tokenized_inputs['input_ids'][i:min(len(final_definitions), i + 4)], output_hidden_states=True)\n",
    "        params['transformer.wte.weight'][displacement + i: displacement + min(len(final_definitions), i + 4),:] = outputs.hidden_states[-1][:,511,:].detach().clone()\n",
    "    model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2G: Batch Add All Embeddings\n",
    "Create updated embeddings for GPT-2 using R2G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2g_embeddings():\n",
    "    # Update definitions to have the CLS token at the beginning\n",
    "    for i in range(len(final_definitions)):\n",
    "        final_definitions[i] = \"[CLS] \" + final_definitions[i]\n",
    "    \n",
    "    # Helpful Debug Message\n",
    "    print(\"Number of total definitions: \" + str(len(final_definitions)))\n",
    "\n",
    "    # Tokenizing all of the definitions at once\n",
    "    predict_tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = predict_tokenizer(final_definitions, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=511)\n",
    "\n",
    "    # Add the new tokens and resize the model embeddings matrix\n",
    "    displacement = len(tokenizer)\n",
    "    tokenizer.add_tokens(final_fake_words)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    params = model.state_dict()\n",
    "\n",
    "    # Adding new embeddings in a range of 4\n",
    "    for i in range(0, len(final_definitions), 4):\n",
    "        outputs = predict_model(input_ids=tokenized_inputs['input_ids'][i:min(len(final_definitions), i + 4)], output_hidden_states=True)\n",
    "        params['transformer.wte.weight'][displacement + i: displacement + min(len(final_definitions), i + 4),:] = outputs.hidden_states[-1][:,0,:].detach().clone()\n",
    "    model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random Initialization: Batch Add All Embeddings\n",
    "Create updated embeddings for GPT-2 using Hewitt random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_embeddings():    \n",
    "    # Helpful Debug Message\n",
    "    print(\"Number of total definitions: \" + str(len(final_definitions)))\n",
    "    \n",
    "    # Use Hewitt code to get embeddings that are average of all other embeddings\n",
    "    params = model.state_dict()\n",
    "    embeddings = params['transformer.wte.weight']\n",
    "    mu = torch.mean(embeddings, dim=0)\n",
    "    n = embeddings.size()[0]\n",
    "    sigma = ((embeddings - mu).T @ (embeddings - mu)) / n\n",
    "    dist = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            mu, covariance_matrix=1e-5*sigma)\n",
    "    \n",
    "    # Generate new embeddings, add new tokens, and resize the model embeddings matrix\n",
    "    new_embeddings = torch.stack(tuple((dist.sample() for _ in range(3))), dim=0)\n",
    "    displacement = len(tokenizer)\n",
    "    tokenizer.add_tokens(final_fake_words)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Adding new embeddings in a range of 4\n",
    "    for i in range(0, len(final_definitions), 4):\n",
    "        params['transformer.wte.weight'][displacement + i: displacement + min(len(final_definitions), i + 4),:] = new_embeddings[displacement + i: displacement + min(len(final_definitions), i + 4),:]\n",
    "    model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check for Maintaining Index Parity\n",
    "Make sure that all of the embeddings, definitions, and fake words are in the same place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Sanity Check #1: at index 41\\nprint(final_fake_words[41])\\nprint(final_definitions[41])\\nprint(tokenizer(\"prodittionzzz\", return_tensors=\"pt\"))\\nprint(tokenizer.decode(displacement + 41))\\nprint(displacement + 41)\\n\\n# Sanity Check #1: at the last index\\nprint(final_fake_words[-1])\\nprint(final_definitions[-1])\\nprint(tokenizer(\"nturyzzz\", return_tensors=\"pt\"))\\nprint(tokenizer.decode(displacement + len(final_fake_words) - 1))\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Sanity Check #1: at index 41\n",
    "print(final_fake_words[41])\n",
    "print(final_definitions[41])\n",
    "print(tokenizer(\"prodittionzzz\", return_tensors=\"pt\"))\n",
    "print(tokenizer.decode(displacement + 41))\n",
    "print(displacement + 41)\n",
    "\n",
    "# Sanity Check #1: at the last index\n",
    "print(final_fake_words[-1])\n",
    "print(final_definitions[-1])\n",
    "print(tokenizer(\"nturyzzz\", return_tensors=\"pt\"))\n",
    "print(tokenizer.decode(displacement + len(final_fake_words) - 1))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure Correct Tokenization of Sample Sentences\n",
    "Make sure all of the sample sentences are correctly tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load in all of the example sentences\n",
    "example_sentences = json.load(open('datasets/example_sentences.json'))\n",
    "\n",
    "# Collecting all of the sentences and words used for training\n",
    "fake_word_to_sentences = defaultdict(list)\n",
    "\n",
    "# Iterate through each example sentence\n",
    "for word in example_sentences:\n",
    "    for sentence in example_sentences[word]:\n",
    "        # Get the corresponding fake word and craft the new sentence\n",
    "        fake_word = final_fake_words[final_real_words.index(word)]\n",
    "        fake_sentence = sentence.replace(word, fake_word) \n",
    "        \n",
    "        # Check if tokenized correctly\n",
    "        tokens = tokenizer(fake_sentence)['input_ids']\n",
    "        word_token = tokenizer(fake_word)['input_ids'][0]\n",
    "        if (word_token in tokens):\n",
    "            fake_word_to_sentences[fake_word].append(fake_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class for Example Sentences\n",
    "Key: makes sure that the training sentences are already a tensor of the tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ExampleSentencesData(Dataset):\n",
    "\n",
    "    def __init__(self, training_sentences, tokenizer):\n",
    "        self.training_sentences = tokenizer(training_sentences, return_tensors=\"pt\", padding='max_length', truncation=True)['input_ids'].to(\"cuda\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sentence': self.training_sentences[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Creating the training loader and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reset the padding token and set up parameters for the training loader\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set up Adam optimizer and parameters for training loader\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train_params = {\n",
    "    'batch_size': 2,        \n",
    "    'shuffle': False,        \n",
    "    'num_workers': 0\n",
    "}\n",
    "\n",
    "# Zero out non embedding layers by setting require_grad = false\n",
    "params = model.state_dict()\n",
    "for param in params:\n",
    "    if (param != \"transformer.wte.weight\"):\n",
    "        params[param].requires_grad = False\n",
    "model.load_state_dict(params)\n",
    "\n",
    "# Iterate through each of the real words\n",
    "embedding_mask = torch.zeros(model.transformer.wte.weight.shape).to(\"cuda\")\n",
    "for key in tqdm(fake_word_to_sentences):\n",
    "    # Set up our dataset and our training oader\n",
    "    special_word_dataset = ExampleSentencesData(fake_word_to_sentences[key], tokenizer)\n",
    "    training_loader = DataLoader(special_word_dataset, **train_params)\n",
    "    \n",
    "    # Enumerate all examples in our training loader\n",
    "    for _ in range(2):\n",
    "        for j, data in enumerate(training_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get the tokens for each of the words in the fake sentence\n",
    "            #sentence_tokens = tokenizer(fake_word_to_sentences[key], return_tensors=\"pt\", padding='max_length', truncation=True)['input_ids']\n",
    "            #cuda_sentence_tokens = sentence_tokens.to('cuda')\n",
    "            \n",
    "            # Run it through the actual model and calculate the loss\n",
    "            outputs = model(input_ids=data['sentence'], labels=data['sentence'])\n",
    "            outputs.loss.backward()\n",
    "            \n",
    "            # Zero out gradient layers\n",
    "            embedding_mask[tokenizer(key)['input_ids'][0]] = 1\n",
    "            model.transformer.wte.weight.grad = model.transformer.wte.weight.grad * embedding_mask\n",
    "            embedding_mask[tokenizer(key)['input_ids'][0]] = 0\n",
    "            \n",
    "            # Take a step with the optimizer\n",
    "            optimizer.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('weights/G2G-Finetuned-2-Epochs-T/tokenizer_config.json',\n",
       " 'weights/G2G-Finetuned-2-Epochs-T/special_tokens_map.json',\n",
       " 'weights/G2G-Finetuned-2-Epochs-T/vocab.json',\n",
       " 'weights/G2G-Finetuned-2-Epochs-T/merges.txt',\n",
       " 'weights/G2G-Finetuned-2-Epochs-T/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('weights/G2G-Finetuned-2-Epochs')\n",
    "tokenizer.save_pretrained('weights/G2G-Finetuned-2-Epochs-T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(99999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
