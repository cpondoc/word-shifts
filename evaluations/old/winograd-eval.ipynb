{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224N Final Project - Evaluating on Winograd Dataset\n",
    "By: Christopher Pondoc, Joseph Guman, and Joseph O'Brien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using GPU: \" + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in GPT-2 Model\n",
    "Using HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Winograd Dataset\n",
    "Also taken from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset winograd_wsc/wsc273 to /home/ubuntu/.cache/huggingface/datasets/winograd_wsc/wsc273/0.0.0/0651311f3b6dda14889d9a063030a02458395ee50ab9f41cca4cd5a89c0c3dce...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030a4ca1a1dc4c478dc160c0f86778b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/273 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset winograd_wsc downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/winograd_wsc/wsc273/0.0.0/0651311f3b6dda14889d9a063030a02458395ee50ab9f41cca4cd5a89c0c3dce. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6420f4461b9842ccadd12116bc19beb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"winograd_wsc\", 'wsc273')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on One Example\n",
    "Writing a function that is reusable and works for one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_winograd(example):\n",
    "    # Change 'the' to lowercase\n",
    "    first_choice, second_choice = example['options'][0], example['options'][1]\n",
    "    if (example['options'][0][:4] == \"The \"):\n",
    "        first_choice = \"the \" + first_choice[4:]\n",
    "    if (example['options'][1][:4] == \"The \"):\n",
    "        second_choice = \"the \" + second_choice[4:]\n",
    "\n",
    "    # First, replace the word with each of the options\n",
    "    first_text, second_text = example['text'], example['text']\n",
    "    first_option = first_text[:example['pronoun_loc']] + first_choice + first_text[example['pronoun_loc'] + len(example['pronoun']):]\n",
    "    second_option = second_text[:example['pronoun_loc']] + second_choice + second_text[example['pronoun_loc'] + len(example['pronoun']):]\n",
    "\n",
    "    # Tokenize each string and produce labels\n",
    "    first_inputs, second_inputs = tokenizer(first_option, return_tensors=\"pt\"), tokenizer(second_option, return_tensors=\"pt\")\n",
    "    \n",
    "    # Create the first token labels\n",
    "    first_masked_tokens = tokenizer(first_text[:example['pronoun_loc']] + first_choice, return_tensors=\"pt\")\n",
    "    first_labels = first_masked_tokens[\"input_ids\"][0]\n",
    "    first_mask = torch.full((1, first_labels.shape[0]), -100)\n",
    "    first_fill = tokenizer(first_text[example['pronoun_loc'] + len(example['pronoun']):], return_tensors=\"pt\")[\"input_ids\"]\n",
    "    final_first_labels = torch.cat((first_mask, first_fill), dim=1)\n",
    "\n",
    "    # Create the second token labels\n",
    "    second_masked_tokens = tokenizer(second_text[:example['pronoun_loc']] + second_choice, return_tensors=\"pt\")\n",
    "    second_labels = second_masked_tokens[\"input_ids\"][0]\n",
    "    second_mask = torch.full((1, second_labels.shape[0]), -100)\n",
    "    second_fill = tokenizer(second_text[example['pronoun_loc'] + len(example['pronoun']):], return_tensors=\"pt\")[\"input_ids\"]\n",
    "    final_second_labels = torch.cat((second_mask, second_fill), dim=1)\n",
    "\n",
    "    # Evaluate the model on each example and check\n",
    "    first_loss = model(**first_inputs, labels=final_first_labels).loss\n",
    "    second_loss = model(**second_inputs, labels=final_second_labels).loss\n",
    "    \n",
    "    # Write down the correct value and check\n",
    "    if (first_loss < second_loss):\n",
    "        return (example['label'] == 0)\n",
    "    else:\n",
    "        return (example['label'] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Winograd on GPT-2\n",
    "Looking specifically at `wsc285`, or the first $285$ examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Large achieved a score of: 0.7106227106227107\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for example in dataset['test']:\n",
    "    correct += evaluate_winograd(example)\n",
    "    \n",
    "print(\"GPT-2 Large achieved a score of: \" + str((float(correct) / float(len(dataset['test'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
