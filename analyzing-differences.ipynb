{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using GPU: \" + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# GPT-2 Model and Tokenizer to be fine-tuned\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium').to(\"cuda\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import CSV and get the real to fake\n",
    "rtf_df = pd.read_csv(\"/home/ubuntu/test/datasets/realtofake.csv\")\n",
    "real_words_list = rtf_df[\"Real\"].tolist()\n",
    "fake_words_list = rtf_df[\"Fake\"].tolist()\n",
    "\n",
    "# Populate dictionary\n",
    "real_to_fake_dict = {}\n",
    "for i in range(len(real_words_list)):\n",
    "    real_to_fake_dict[real_words_list[i]] = fake_words_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Keep track of all the final definitions\n",
    "final_definitions = []\n",
    "\n",
    "# Loop through each real word and append\n",
    "for word in real_words_list:\n",
    "    definition = \"\"\n",
    "    for synset in wn.synsets(word):\n",
    "        definition += synset.definition() + \". \"\n",
    "    final_definitions.append(definition)\n",
    "\n",
    "# Quick sanity check\n",
    "assert(len(real_words_list) == len(final_definitions))\n",
    "assert(len(fake_words_list) == len(final_definitions))\n",
    "assert(len(real_to_fake_dict) == len(final_definitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Hewitt code to get embeddings that are average of all other embeddings\n",
    "params = model.state_dict()\n",
    "embeddings = params['transformer.wte.weight']\n",
    "mu = torch.mean(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model = GPT2LMHeadModel.from_pretrained(\"/home/ubuntu/test/weights/G2GMaskingBestM\").to(\"cuda\")\n",
    "predict_tokenizer = GPT2Tokenizer.from_pretrained(\"/home/ubuntu/test/weights/G2GMaskingT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total definitions: 343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helpful Debug Message\n",
    "print(\"Number of total definitions: \" + str(len(final_definitions)))\n",
    "\n",
    "# Tokenizing all of the definitions at once\n",
    "tokenized_inputs = predict_tokenizer(final_definitions, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=511)\n",
    "tokenized_cls = predict_tokenizer([\" [CLS]\"] * len(final_definitions), return_tensors=\"pt\")\n",
    "\n",
    "# Get the correct input IDs and and attention mask\n",
    "tokenized_inputs['input_ids'] = torch.cat((tokenized_inputs['input_ids'], tokenized_cls['input_ids']), dim=1).to(\"cuda\")\n",
    "tokenized_inputs['attention_mask'] = torch.cat((tokenized_inputs['attention_mask'], tokenized_cls['attention_mask']), dim=1).to(\"cuda\")\n",
    "\n",
    "# Add the new tokens and resize the model embeddings matrix\n",
    "displacement = len(tokenizer)\n",
    "tokenizer.add_tokens(fake_words_list)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "params = model.state_dict()\n",
    "\n",
    "# Adding new embeddings in a range of 4\n",
    "for i in range(0, len(final_definitions), 4):\n",
    "    outputs = predict_model(input_ids=tokenized_inputs['input_ids'][i:min(len(final_definitions), i + 4)], output_hidden_states=True, attention_mask=tokenized_inputs['attention_mask'][i:min(len(final_definitions), i + 4)])\n",
    "    params['transformer.wte.weight'][displacement + i: displacement + min(len(final_definitions), i + 4),:] = outputs.hidden_states[-1][:,511,:].detach().clone()\n",
    "model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del predict_model\n",
    "del predict_tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.state_dict()\n",
    "embeddings = params['transformer.wte.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n",
      "tensor([-0.0558, -0.0170, -0.0460,  ...,  0.0417, -0.0542, -0.0225],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "embeddings = embeddings[50257:]\n",
    "print(len(embeddings))\n",
    "\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor(0.0014, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "print()\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    total_loss += mse_loss(mu,embeddings[i])\n",
    "print(total_loss/len(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6543, device='cuda:0')\n",
      "tensor(2.0444, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "predicted_mu = torch.mean(embeddings, dim=0)\n",
    "print(torch.linalg.vector_norm(predicted_mu))\n",
    "print(torch.linalg.vector_norm(mu))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
